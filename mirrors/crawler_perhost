#!/usr/bin/python
import pkg_resources
pkg_resources.require("TurboGears")

from sqlobject import *
import sys, os, string
import turbogears
from mirrors.model import *
from datetime import datetime
import ftplib
from ftplib import FTP
import httplib
from urlparse import urlsplit
from optparse import OptionParser

from turbogears.database import PackageHub


################################################
# overrides for httplib because we're
# handling keepalives ourself
################################################
class myHTTPResponse(httplib.HTTPResponse):
    def begin(self):
        httplib.HTTPResponse.begin(self)
        self.will_close=False

    def isclosed(self):
        """This is a hack, because otherwise httplib will fail getresponse()"""
        return True

    def keepalive_ok(self):
        # HTTP/1.1 connections stay open until closed
        if self.version == 11:
            ka = self.msg.getheader('connection')
            if ka and "close" in ka.lower():
                return False
            else:
                return True

        # other HTTP connections may have a connection: keep-alive header
        ka = self.msg.getheader('connection')
        if ka and "keep-alive" in ka.lower():
            return True

        try:
            ka = self.msg.getheader('keep-alive')
            if ka is not None:
                maxidx = ka.index('max=')
                max = ka[maxidx+4:]
                if max == '1':
                    return False
                return True
            else:
                ka = self.msg.getheader('connection')
                if ka and "keep-alive" in ka.lower():
                    return True
                return False
        except:
            return False
        return False

class myHTTPConnection(httplib.HTTPConnection):
    response_class=myHTTPResponse
    
    def end_request(self):
        self.__response = None


################################################
# the magic begins

class hostState:
    def __init__(self, http_debuglevel=0, ftp_debuglevel=0):
        self.httpconn = {}
        self.ftpconn = {}
        self.http_debuglevel = http_debuglevel
        self.ftp_debuglevel = ftp_debuglevel
        self.ftp_dir_results = None


    def get_connection(self, url):
        scheme, netloc, path, query, fragment = urlsplit(url)
        if scheme == 'ftp':
            if self.ftpconn.has_key(netloc):
                return self.ftpconn[netloc]
        elif scheme == 'http':
            if self.httpconn.has_key(netloc):
                return self.httpconn[netloc]
        return None


    def open_http(self, url):
        scheme, netloc, path, query, fragment = urlsplit(url)
        if not self.httpconn.has_key(netloc):
            self.httpconn[netloc] = myHTTPConnection(netloc)
            self.httpconn[netloc].set_debuglevel(self.http_debuglevel)
        return self.httpconn[netloc]

    def _open_ftp(self, netloc):
        if not self.ftpconn.has_key(netloc):
            self.ftpconn[netloc] = FTP(netloc)
            self.ftpconn[netloc].set_debuglevel(self.ftp_debuglevel)
            self.ftpconn[netloc].login()

    def check_ftp_dir_callback(self, line):
        if self.ftp_debuglevel > 0:
            print line
        self.ftp_dir_results.append(line)

    def ftp_dir(self, url):
        if not url.endswith('/'):
            url += '/'
            scheme, netloc, path, query, fragment = urlsplit(url)
            self._open_ftp(netloc)
            c = self.ftpconn[netloc]
            self.ftp_dir_results = []
            c.dir(path, self.check_ftp_dir_callback)


    def close_http(self, url):
        scheme, netloc, path, query, fragment = urlsplit(url)
        if self.httpconn.has_key(netloc):
            self.httpconn[netloc].close()
            del self.httpconn[netloc]

    def close_ftp(self, url):
        scheme, netloc, path, query, fragment = urlsplit(url)
        if self.ftpconn.has_key(netloc):
            self.ftpconn[netloc].quit()
            del self.ftpconn[netloc]

    def close(self):
        for c in self.httpconn.keys():
            self.close_http(c)

        for c in self.ftpconn.keys():
            self.close_ftp(c)



def get_ftp_dir(hoststate, url):
    try:
        hoststate.ftp_dir(url)
    except ftplib.error_temp, e:
        # Returned by Boston University when directory does not exist
        if e == 450:
            return []
        # Returned by Prinston University when cannot log in due to connection restrictions
        if e == 421:
            return None
    except:
            raise
    return hoststate.ftp_dir_results

def check_ftp_file(hoststate, url, filedata):
    results = get_ftp_dir(hoststate, url)
    if results is None:
        return None
    if len(results) == 1:
        line = results[0].split()
        if line[4] == filedata['size']:
            return True
    return False

def check_url(hoststate, url, filedata):
    if url.startswith('http:'):
        return check_head(hoststate, url, filedata)
    elif url.startswith('ftp:'):
        return check_ftp_file(hoststate, url, filedata)


class HTTPUnknown(Exception): pass
class HTTP500(Exception): pass

def handle_redirect(hoststate, url, location, filedata):
    if location.startswith('/'):
        scheme, netloc, path, query, fragment = urlsplit(url)
        location = '%s:%s%s' % (scheme, netloc, location)
    return check_url(hoststate, location, filedata)


def check_head(hoststate, url, filedata):
    """ Returns tuple:
    True - URL exists
    False - URL doesn't exist
    None - we don't know
    """
    
    try:
        conn = hoststate.open_http(url)
    except:
        return None
    
    conn.request('HEAD', url,
                 headers={'Connection':'Keep-Alive',
                          'Pragma':'no-cache',
                          'User-Agent':'mirrormanager-crawler/0.1 (+http://hosted.fedoraproject.org/projects/mirrormanager)'})
    
    try:
        r = conn.getresponse()
        status = r.status
    except:
        raise HTTPUnknown()

    conn.end_request()
    if not r.keepalive_ok():
        hoststate.close_http(url)

    content_length = r.getheader('Content-Length')
    #last_modified  = r.getheader('Last-Modified')

    if status >= 200 and status < 300:
        # fixme should check last_modified too
        if filedata['size'] == content_length:
            return True
        else:
            return False
    if status >= 300 and status < 400:
        return handle_redirect(hoststate, url, r.getheader('Location'), filedata)
    if status >= 400 and status < 500:
        # all the 403 Forbidden and 404 Not Founds
        return False
    if status >= 500:
        raise HTTP500()

    print "status = %s" % status
    raise HTTPUnknown()


def sync_hcds(host, host_category_dirs):
    current_hcds = {}
    for (hc, d), up2date in host_category_dirs.iteritems():
        if up2date is None:
            continue

        topname = hc.category.topdir.name
        path = d.name[len(topname)+1:]

        hcd = HostCategoryDir.selectBy(host_category=hc, path=path)
        if hcd.count() > 0:
            hcd = hcd[0]
        else:
            hcd = HostCategoryDir(host_category=hc, path=path,
                                  lastCrawled=datetime.utcnow(), up2date=True)

        hcd.lastCrawled=datetime.utcnow()
        hcd.up2date=up2date
        current_hcds[hcd] = True
        hcd.sync()

    # now-historical HostCategoryDirs are not up2date
    # we wait for a cascading Directory delete to delete this
    # (though it won't right now...)
    for hc in host.categories:
        for hcd in hc.dirs:
            try:
                thcd = current_hcds[hcd]
            except KeyError:
                hcd.lastCrawled=datetime.utcnow()
                hcd.up2date=False
                hcd.sync()
    

def method_pref(urls):
    pref = None
    for u in urls:
        if u.startswith('http:'):
            pref = u
            break
    if pref is None:
        for u in urls:
            if u.startswith('ftp:'):
                pref = u
                break
    return pref
        

def add_parents(host_category_dirs, hc, d):
    splitpath = d.name.split('/')
    if len(splitpath[:-1]) > 0:
        parent = '/'.join(splitpath[:-1])
        try:
            hcd = host_category_dirs[(hc, parent)]
        except KeyError:
            try:
                parentDir = Directory.byName(parent)
                host_category_dirs[(hc, parentDir)] = True
            except SQLObjectNotFound: # recursed out of the directory structure
                parentDir = None
                
        if parentDir and parentDir != hc.category.topdir: # stop at top of the category
            return add_parents(host_category_dirs, hc, parentDir)
    
    return host_category_dirs


def try_perfile(d, hoststate, url):
    exists = None
    for file in d.files.keys():
        exists = None
        graburl = "%s/%s" % (url, file)
        try:
            exists = check_url(hoststate, graburl, d.files[file])
            #print "%s %s" % (exists, graburl)
            if exists == False:
                return False
        except ftplib.all_errors:
            hoststate.close_ftp(url)
            return None
        except:
            return None

    if exists is None:
        return None

    return True


def try_perdir(d, hoststate, url):
    if not url.startswith('ftp'):
        return None
    results = {}
    listing = get_ftp_dir(hoststate, url)
    if listing is None:
        return None

    if len(listing) == 0:
        #print 'FALSE %s' % url
        return False
    
    for line in listing:
        fields = line.split()
        results[fields[8]] = {'size': fields[4]}

    for file in d.files.keys():
        try:
            if results[file]['size'] != d.files[file]['size']:
                return False
        except:
            return False
    return True
        
    

def per_host(host, options=None):
    host = Host.get(host)
    done_host=False
    host_category_dirs = {}
    if host.private and not options.include_private:
        return
    # Don't crawl Red Hat sites for now
    if host.site.name == "Red Hat":
        return
    http_debuglevel = 0
    ftp_debuglevel = 0
    if options.debug_http:
        http_debuglevel = 2
    if options.debug_ftp:
        ftp_debuglevel = 2

    hoststate = hostState(http_debuglevel=http_debuglevel, ftp_debuglevel=ftp_debuglevel)
    for hc in host.categories:
        category = hc.category
        trydirs = category.directories
        categoryUrl = method_pref(host.category_urls(category.name))
        if categoryUrl is None:
            continue
        categoryPrefixLen = len(category.topdir.name)+1
        
        for d in trydirs:
            dirname = d.name[categoryPrefixLen:]
            url = '%s/%s' % (categoryUrl, dirname)

            has_all_files = try_perdir(d, hoststate, url)
            if has_all_files is None:
                has_all_files = try_perfile(d, hoststate, url)


            if has_all_files == False:
                host_category_dirs[(hc, d)] = False
                for t in trydirs:
                    if d == t or t.name.startswith("%s/" % d.name):
                        trydirs.remove(t)
            elif has_all_files == True:                
                host_category_dirs[(hc, d)] = True
                print url
                # make sure our parent dirs appear on the list too
                host_category_dirs = add_parents(host_category_dirs, hc, d)
            elif has_all_files is None:
                host_category_dirs[(hc, d)] = None

    hoststate.close()
    if len(host_category_dirs) > 0:
        sync_hcds(host, host_category_dirs)


def main():
    parser = OptionParser(usage=sys.argv[0] + " [options]")
    parser.add_option("-c", "--config",
                      dest="config", default='dev.cfg',
                      help="TurboGears config file to use")

    parser.add_option("--hostname",
                      dest="hostname", default=None,
                      help="Crawl a single host at site")

    parser.add_option("--site",
                      dest="site", default=None,
                      help="Crawl a single host at site")

    parser.add_option("--debug-http",
                      action="store_true", dest="debug_http", default=False,
                      help="Dump HTTP headers for each transaction")

    parser.add_option("--debug-ftp",
                      action="store_true", dest="debug_ftp", default=False,
                      help="Dump FTP headers for each transaction")

    parser.add_option("--include-private",
                      action="store_true", dest="include_private", default=False,
                      help="Include hosts marked 'private' in the crawl")

    (options, args) = parser.parse_args()

    turbogears.update_config(configfile=options.config,
                             modulename="mirrors.config")
    global hub
    global __connection__
    hub = PackageHub("mirrors")
    __connection__ = hub
    
    os.chdir('/tmp')

    try:
        site = Site.byName(options.site)
        host = Host.selectBy(name=options.hostname, site=site)[0]
    except:
        print "Site %s Host %s not found." % (options.site, options.hostname)
        sys.exit(1)

    per_host(host.id, options=options)



if __name__ == "__main__":
    sys.exit(main())
        
