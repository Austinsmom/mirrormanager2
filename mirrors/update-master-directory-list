#!/usr/bin/python
import pkg_resources
pkg_resources.require("TurboGears")

from sqlobject import *
import sys, os, string
import turbogears
from mirrors.model import *
from mirrors.repomap import *
import re
import glob

# look on the command line for a desired config file
if len(sys.argv) > 1:
    turbogears.update_config(configfile=sys.argv[1], 
        modulename="mirrors.config")
else:
    print "usage: update-master-directory-list dev.cfg"
    sys.exit(1)
    

from turbogears.database import PackageHub
hub = PackageHub("mirrors")
__connection__ = hub

def trim_os_from_dirname(dirname):
    # trim the /os off the name
    index = dirname.rfind('/os')
    if index > 0:
        dirname = dirname[:index]
    return dirname

def rename_SRPMS_source(l):
    rc = []
    for i in l:
        if i == 'source':
            pass
        elif i == 'SRPMS':
            rc.append('source')
        else:
            rc.append(i)
    return rc


def _get_version_from_path(path):
    s = r'/(([\.\d]+)(\-\w+)?)/'
    m = re.search(re.compile(s), path)
    return m.group(1)

def create_version_from_path(category, path):
    ver = None
    vname = _get_version_from_path(path)
    if vname is not None and vname != '':
        if '/test/' in path:
            isTest = True
        else:
            isTest = False
        ver = Version(product=category.product, name=vname, isTest=isTest)

    return ver

def guess_ver_arch_from_path(category, path):
    arch = None
    if 'SRPMS' in path:
        arch = Arch.byName('source')
    else:
        for a in Arch.select():
            s = '.*(^|/)%s(/|$).*' % (a.name)
            if re.compile(s).match(path):
                arch = a
                break

    ver = None
    for v in Version.selectBy(product=category.product):
        s = '.*(^|/)%s(/|$).*' % (v.name)
        if re.compile(s).match(path):
            ver = v
            break

    # create Versions if we can figure it out...
    if ver is None:
        ver = create_version_from_path(category, path)
        
    return (ver, arch)


from mirrors.repomap import *
import sha
import md5
import yum.repoMDObject

# Something like this is committed to yum upstream, but may not be in the copy we are using.
def set_repomd_timestamp(yumrepo):
    timestamp = 0
    for ft in yumrepo.fileTypes():
        thisdata = yumrepo.repoData[ft]
        timestamp = max(int(thisdata.timestamp), timestamp)
    yumrepo.timestamp = timestamp
    return timestamp

def make_file_details_from_checksums(dir):
    def _parse_checksum_file(path):
        r = {}
        try:
            f = open(path, 'r')
            for line in f.readlines():
                line = line.strip()
                s = line.split()
                if len(s) < 2:
                    continue
                r[s[1]] = s[0]
            f.close()
        except:
            pass
        return r

    def _checksums_from_globs(dirname, g):
        d = {}
        checksum_files = []
        for f in g:
            checksum_files.extend(glob.glob(os.path.join(dirname, f)))
        for f in checksum_files:
            d.update(_parse_checksum_file(f))
        return d

    sha1_globs = ['*.sha1sum', 'SHA1SUM']
    md5_globs = ['*.md5sum', 'MD5SUM']
    md5dict = _checksums_from_globs(dir.name, md5_globs)
    sha1dict = _checksums_from_globs(dir.name, sha1_globs)

    files = set()
    for k in md5dict.keys():
        files.add(k)
    for k in sha1dict.keys():
        files.add(k)

    for f in files:
        sha1 = sha1dict.get(f)
        md5  = md5dict.get(f)
        s = os.stat(os.path.join(dir.name, f))
        size = s.st_size
        mtime = s.st_mtime
        try:
            fd = FileDetail.selectBy(directory=dir, filename=f)[0]
            if fd.sha1 != sha1:
                fd.sha1 = sha1
            if fd.md5 != md5:
                fd.md5 = md5
            if fd.size != size:
                fd.size = size
            if fd.mtime != mtime:
                fd.mtime = mtime
        except:
            fd = FileDetail(directory=dir, filename=f, sha1=sha1, md5=md5, timestamp=mtime, size=size)
    


def make_repomd_file_details(dir):
    repodataDir = os.path.join(dir.name, 'repodata')
    fname = os.path.join(dir.name, 'repodata', 'repomd.xml')
    if not os.path.exists(fname):
        return
    d = Directory.byName(repodataDir)
    try:
        f = open(fname, 'r')
        repomd = read()
        f.close()
    except:
        return
    sha1 = sha.new(repomd)
    md5 = md5.new(repomd)

    yumrepo = yum.repoMDObject.RepoMD('repoid', repomd_fname)
    if 'timestamp' not in yumrepo.__dict__:
        set_repomd_timestamp(yumrepo)
    ts = datetime.datetime.fromtimestamp(yumrepo.timestamp)
    try:
        yr = FileDetail(directory=d, filename='repomd.xml', sha1=sha1.hexdigest(), md5=md5.hexdigest(), timestamp=ts, size=len(repomd))
    except:
        pass


def make_repository(dir, category):
    repo = None
    path = dir.name[len(category.topdir.name)+1:]
    (ver, arch) = guess_ver_arch_from_path(category, path)
    path = trim_os_from_dirname(path)
    name=path.split('/')
    name = rename_SRPMS_source(name)
    name='-'.join(name)
    name='%s-%s-%s' % (category.product.name, category.name, name)

    prefix = repo_prefix(path, category, ver)
    try:
        repo = Repository(name=name, category=category, version=ver, arch=arch, directory=dir, prefix=prefix)
    except:
        pass

    return repo

    


def nuke_gone_directories(category, category_directories):
    """ deleting a Directory has a ripple effect through the whole
        database.  Be really sure you're ready do to this.  It comes
        in handy when say a Test release is dropped."""
        
    for d in Directory.select():
        if len(d.categories == 1) and category in d.categories:
            if not category_directories.has_key(d.name):
                d.destroySelf()

unreadable_dirs = {}

def parent_dir(path):
    sdir = path.split('/')[:-1]
    return '/'.join(sdir)

def make_one_directory(line, category, path, category_directories):
    global unreadble_dirs
    readable=True
    d = line.split()[4]
    if re.compile('^\.$').match(d):
        dname = path
    else:
        dname = "%s/%s" % (path, d)
    perms = line.split()[0]
    if not re.compile('^d......r.x').match(perms) or parent_dir(dname) in unreadable_dirs:
        readable=False
        unreadable_dirs[dname] = True

    category_directories[dname] = {'files':{}, 'isRepository':False, 'readable':readable}
    if d.endswith('repodata'):
        parent_dname = dname[:-len('/repodata')]
        try:
            category_directories[parent_dname]['isRepository'] = True
        except KeyError:
            category_directories[parent_dname] = {'files':{}, 'isRepository':True, 'readable':readable}
        
            
    return dname, category_directories

def add_file_to_directory(line, dname, path, category_directories):
    perm, size, date, time, filepath = line.split()
    year, month, day = date.split('/')
    hour, minute, second = time.split(':')
    dt = datetime(int(year), int(month), int(day), int(hour), int(minute), int(second))
    l = filepath.split('/')
    filename = l[-1]
    subpath = l[:-1]
    if len(subpath) > 0:
        dirpath = ("%s/" % path) + '/'.join(subpath)
    else:
        dirpath = path
    category_directories[dirpath]['files'][filename] = {'size':size,
                                                        'stat':dt}

def short_filelist(files):
    html=0
    rpms=0
    hdrs=0
    for f in files.keys():
        if f.endswith('.html'): html=html+1
        if f.endswith('.rpm'):  rpms=rpms+1
        if f.endswith('.hdr'):  hdrs=hdrs+1
    if html>10 or rpms > 10 or hdrs > 10:
        date_file_list = []
        rc = {}
        for k in files.keys():
            date_file_tuple = (files[k]['stat'], k, files[k]['size'])
            date_file_list.append(date_file_tuple)
            date_file_list.sort()
            date_file_list = date_file_list[-10:]
        
        for stat, k, size in date_file_list:
            rc[k] = files[k]
        return rc
    else:
        return files

def sync_category_directories(category, category_directories):
    for dirpath, value in category_directories.iteritems():
        try:
            dir = Directory.byName(dirpath)
            if dir.readable != value['readable']:
                dir.readable = value['readable']
        except SQLObjectNotFound:
            dir = Directory(name=dirpath,readable=value['readable'])
            dir.addCategory(category)
        if dir.files != short_filelist(value['files']):
            dir.files = short_filelist(value['files'])
        make_file_details_from_checksums(dir)

    # this has to be a second pass to be sure the child repodata/ dir is created in the db first
    for dirpath, value in category_directories.iteritems():
        if value['isRepository']:
            dir = Directory.byName(dirpath)
            make_repository(dir, category)
                make_repomd_file_details(dir)
    ageFileDetails()

def parse_rsync_listing(cname, f):
    category = Category.byName(cname)
    category_directories = {}
    for line in f.readlines():
        line.strip()
        if line.startswith('d'):
            if re.compile('^\.$').match(line):
                # we know the top-level category directory already exists, don't try to re-make it
                pass
            else:
                dname, category_directories = make_one_directory(line, category, category.topdir.name, category_directories)
        else:
            add_file_to_directory(line, dname, category.topdir.name, category_directories)
    sync_category_directories(category, category_directories)



def sync_directories_using_rsync(rsyncpath, cname):
    cmd = 'rsync -r --exclude=.snapshot ' + rsyncpath
    try:
        f = os.popen(cmd)
    except:
        print "Unable to parse category %s rsyncpath %s" % (cname, rsyncpath)
        return
    parse_rsync_listing(cname, f)
    f.close()

def sync_directories_from_file(filename, cname):
    f = open(filename, 'r')
    parse_rsync_listing(cname, f)
    f.close()


# fixme this is quick and dirty
#sync_directories_from_file('/tmp/fedora-linux-core.txt', 'Fedora Core')
#sync_directories_from_file('/tmp/fedora-linux-extras.txt', 'Fedora Extras')
sync_directories_from_file('/tmp/fedora-epel.txt', 'Fedora EPEL')
sync_directories_from_file('/tmp/fedora-web.txt', 'Fedora Web')
sync_directories_from_file('/tmp/fedora-linux.txt', 'Fedora Linux')
sync_directories_using_rsync('rsync://secondary1.fedora.phx.redhat.com/fedora-secondary/', 'Fedora Secondary Arches')
sync_directories_using_rsync('rsync://secondary1.fedora.phx.redhat.com/alt/', 'Fedora Other')
